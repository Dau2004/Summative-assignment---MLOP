#!/usr/bin/env python3
"""
Container Performance Demo
Shows how model performance scales with different numbers of Docker containers
"""

import subprocess
import time
import requests
import json
import asyncio
import aiohttp
import numpy as np
from PIL import Image
import io
from datetime import datetime
import threading

class ContainerPerformanceDemo:
    def __init__(self):
        self.results = {}
        
    def generate_test_image(self):
        """Generate a synthetic test image"""
        weather_types = ['sunny', 'rainy', 'cloudy', 'stormy']
        weather_type = np.random.choice(weather_types)
        
        if weather_type == 'sunny':
            img_array = np.random.randint(200, 255, (128, 128, 3), dtype=np.uint8)
        elif weather_type == 'rainy':
            img_array = np.random.randint(50, 120, (128, 128, 3), dtype=np.uint8)
        elif weather_type == 'cloudy':
            gray_val = np.random.randint(120, 180, (128, 128))
            img_array = np.stack([gray_val, gray_val, gray_val], axis=2)
        else:  # stormy
            img_array = np.random.randint(20, 80, (128, 128, 3), dtype=np.uint8)
        
        img = Image.fromarray(img_array.astype('uint8'), 'RGB')
        img_bytes = io.BytesIO()
        img.save(img_bytes, format='JPEG', quality=85)
        img_bytes.seek(0)
        
        return img_bytes.getvalue()
    
    def check_container_health(self, port):
        """Check if container is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/status", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def start_containers(self, num_containers):
        """Start specified number of containers"""
        print(f"🚀 Starting {num_containers} container(s)...")
        
        # Stop any existing containers
        subprocess.run([\"docker-compose\", \"down\"], capture_output=True, cwd=\".\")
        time.sleep(2)
        
        if num_containers == 1:
            # Start single container on port 8001
            cmd = [\"docker-compose\", \"up\", \"-d\", \"weather-api-1\"]\n            test_port = 8001\n        elif num_containers == 2:\n            # Start two containers\n            cmd = [\"docker-compose\", \"up\", \"-d\", \"weather-api-1\", \"weather-api-2\"]\n            test_port = 8001  # Test against first container\n        else:\n            # Start all containers with nginx load balancer\n            cmd = [\"docker-compose\", \"up\", \"-d\"]\n            test_port = 8000  # Test against nginx load balancer\n        \n        # Build and start containers\n        print(\"   Building Docker images...\")\n        build_result = subprocess.run([\"docker-compose\", \"build\"], capture_output=True, text=True, cwd=\".\")\n        if build_result.returncode != 0:\n            print(f\"   ❌ Build failed: {build_result.stderr}\")\n            return False, None\n        \n        print(\"   Starting containers...\")\n        start_result = subprocess.run(cmd, capture_output=True, text=True, cwd=\".\")\n        if start_result.returncode != 0:\n            print(f\"   ❌ Start failed: {start_result.stderr}\")\n            return False, None\n        \n        # Wait for containers to be healthy\n        print(\"   Waiting for containers to be ready...\")\n        max_wait = 60\n        wait_time = 0\n        \n        while wait_time < max_wait:\n            if self.check_container_health(test_port):\n                print(f\"   ✅ Container(s) ready on port {test_port}!\")\n                return True, test_port\n            \n            print(f\"   ⏳ Waiting... ({wait_time}s/{max_wait}s)\")\n            time.sleep(5)\n            wait_time += 5\n        \n        print(f\"   ❌ Timeout waiting for containers\")\n        return False, None\n    \n    async def run_load_test(self, url, num_requests=30, concurrent_requests=10):\n        \"\"\"Run load test against the containers\"\"\"\n        print(f\"   🔥 Load testing: {num_requests} requests, {concurrent_requests} concurrent\")\n        \n        results = []\n        response_times = []\n        success_count = 0\n        error_count = 0\n        \n        async def send_request(session, request_id):\n            image_data = self.generate_test_image()\n            data = aiohttp.FormData()\n            data.add_field('image', image_data, filename=f'test_{request_id}.jpg', content_type='image/jpeg')\n            \n            start_time = time.time()\n            try:\n                async with session.post(f\"{url}/predict\", data=data) as response:\n                    response_time = (time.time() - start_time) * 1000\n                    \n                    if response.status == 200:\n                        result = await response.json()\n                        return {\n                            'status': 'success',\n                            'response_time': response_time,\n                            'prediction': result.get('prediction', 'unknown')\n                        }\n                    else:\n                        return {\n                            'status': 'error',\n                            'response_time': response_time,\n                            'error': f\"HTTP {response.status}\"\n                        }\n            except Exception as e:\n                response_time = (time.time() - start_time) * 1000\n                return {\n                    'status': 'error',\n                    'response_time': response_time,\n                    'error': str(e)\n                }\n        \n        # Create semaphore for concurrency control\n        semaphore = asyncio.Semaphore(concurrent_requests)\n        \n        async def bounded_request(session, request_id):\n            async with semaphore:\n                return await send_request(session, request_id)\n        \n        start_time = time.time()\n        \n        # Run the test\n        connector = aiohttp.TCPConnector(limit=concurrent_requests * 2)\n        timeout = aiohttp.ClientTimeout(total=30)\n        \n        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n            tasks = [bounded_request(session, i) for i in range(num_requests)]\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        total_time = time.time() - start_time\n        \n        # Process results\n        for result in results:\n            if isinstance(result, dict):\n                if result['status'] == 'success':\n                    success_count += 1\n                    response_times.append(result['response_time'])\n                else:\n                    error_count += 1\n        \n        # Calculate metrics\n        rps = len(results) / total_time if total_time > 0 else 0\n        avg_response = np.mean(response_times) if response_times else 0\n        p95_response = np.percentile(response_times, 95) if len(response_times) > 1 else avg_response\n        success_rate = success_count / len(results) * 100 if results else 0\n        \n        return {\n            'total_requests': len(results),\n            'successful_requests': success_count,\n            'failed_requests': error_count,\n            'total_time': total_time,\n            'requests_per_second': rps,\n            'avg_response_time': avg_response,\n            'p95_response_time': p95_response,\n            'success_rate': success_rate,\n            'response_times': response_times\n        }\n    \n    def stop_containers(self):\n        \"\"\"Stop all containers\"\"\"\n        print(\"   🧹 Stopping containers...\")\n        subprocess.run([\"docker-compose\", \"down\"], capture_output=True, cwd=\".\")\n        time.sleep(2)\n    \n    async def run_comprehensive_demo(self):\n        \"\"\"Run comprehensive performance demo\"\"\"\n        print(\"🎯 CONTAINER PERFORMANCE DEMONSTRATION\")\n        print(\"=\" * 60)\n        print(\"This demo shows how the weather model scales with different container counts\")\n        print()\n        \n        # Test configurations: (containers, requests, concurrent)\n        test_configs = [\n            (1, 30, 8),   # Single container\n            (2, 30, 12),  # Two containers  \n            (3, 30, 15),  # Three containers with load balancer\n        ]\n        \n        all_results = {}\n        \n        for containers, requests, concurrent in test_configs:\n            print(f\"\\n📊 TESTING {containers} CONTAINER(S)\")\n            print(\"-\" * 40)\n            \n            # Start containers\n            success, port = self.start_containers(containers)\n            if not success:\n                print(f\"   ❌ Skipping {containers} container test due to startup failure\")\n                continue\n            \n            # Determine test URL\n            if containers >= 3:\n                test_url = \"http://localhost:8000\"  # Nginx load balancer\n                print(f\"   🌐 Testing via load balancer: {test_url}\")\n            else:\n                test_url = f\"http://localhost:{port}\"\n                print(f\"   🎯 Testing direct connection: {test_url}\")\n            \n            # Wait a bit for stabilization\n            time.sleep(3)\n            \n            # Run load test\n            test_results = await self.run_load_test(test_url, requests, concurrent)\n            all_results[containers] = test_results\n            \n            # Display immediate results\n            print(f\"   ✅ Results:\")\n            print(f\"      RPS: {test_results['requests_per_second']:.1f}\")\n            print(f\"      Avg Response: {test_results['avg_response_time']:.0f}ms\")\n            print(f\"      Success Rate: {test_results['success_rate']:.1f}%\")\n            \n            # Stop containers\n            self.stop_containers()\n        \n        # Generate comparison report\n        self.generate_comparison_report(all_results)\n        \n        return all_results\n    \n    def generate_comparison_report(self, results):\n        \"\"\"Generate performance comparison report\"\"\"\n        print(f\"\\n\\n🏆 PERFORMANCE COMPARISON REPORT\")\n        print(\"=\" * 60)\n        \n        if not results:\n            print(\"❌ No results to compare\")\n            return\n        \n        print(\"| Containers | RPS   | Avg Response | P95 Response | Success Rate |\")\n        print(\"|------------|-------|--------------|--------------|--------------|\")\n        \n        for containers in sorted(results.keys()):\n            result = results[containers]\n            print(f\"| {containers:10d} | {result['requests_per_second']:5.1f} | \"\n                  f\"{result['avg_response_time']:8.0f}ms | \"\n                  f\"{result['p95_response_time']:8.0f}ms | \"\n                  f\"{result['success_rate']:8.1f}% |\")\n        \n        # Analysis\n        print(f\"\\n📈 SCALING ANALYSIS:\")\n        \n        if len(results) >= 2:\n            container_counts = sorted(results.keys())\n            \n            # Compare 1 vs 2 containers\n            if 1 in results and 2 in results:\n                rps_improvement = (results[2]['requests_per_second'] / results[1]['requests_per_second'] - 1) * 100\n                latency_change = (results[2]['avg_response_time'] / results[1]['avg_response_time'] - 1) * 100\n                \n                print(f\"   1→2 containers: {rps_improvement:+.1f}% RPS, {latency_change:+.1f}% latency\")\n            \n            # Compare 2 vs 3 containers\n            if 2 in results and 3 in results:\n                rps_improvement = (results[3]['requests_per_second'] / results[2]['requests_per_second'] - 1) * 100\n                latency_change = (results[3]['avg_response_time'] / results[2]['avg_response_time'] - 1) * 100\n                \n                print(f\"   2→3 containers: {rps_improvement:+.1f}% RPS, {latency_change:+.1f}% latency\")\n        \n        # Find best configuration\n        best_rps = max(results.items(), key=lambda x: x[1]['requests_per_second'])\n        best_latency = min(results.items(), key=lambda x: x[1]['avg_response_time'])\n        \n        print(f\"\\n🥇 BEST PERFORMANCE:\")\n        print(f\"   Highest RPS: {best_rps[1]['requests_per_second']:.1f} with {best_rps[0]} container(s)\")\n        print(f\"   Lowest Latency: {best_latency[1]['avg_response_time']:.0f}ms with {best_latency[0]} container(s)\")\n        \n        # Save results\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"load_test_results/container_performance_{timestamp}.json\"\n        \n        with open(filename, 'w') as f:\n            json.dump({\n                'timestamp': datetime.now().isoformat(),\n                'results': results,\n                'summary': {\n                    'best_rps': {'containers': best_rps[0], 'value': best_rps[1]['requests_per_second']},\n                    'best_latency': {'containers': best_latency[0], 'value': best_latency[1]['avg_response_time']}\n                }\n            }, f, indent=2)\n        \n        print(f\"\\n💾 Detailed results saved to: {filename}\")\n\nasync def main():\n    \"\"\"Run the container performance demonstration\"\"\"\n    demo = ContainerPerformanceDemo()\n    \n    try:\n        # Check if Docker is available\n        result = subprocess.run([\"docker\", \"--version\"], capture_output=True)\n        if result.returncode != 0:\n            print(\"❌ Docker is not available. Please install Docker first.\")\n            return\n        \n        print(\"✅ Docker is available\")\n        \n        # Run the comprehensive demo\n        results = await demo.run_comprehensive_demo()\n        \n        print(f\"\\n🎉 DEMONSTRATION COMPLETED!\")\n        print(f\"📊 Tested {len(results)} different container configurations\")\n        \n    except KeyboardInterrupt:\n        print(\"\\n🛑 Demo interrupted by user\")\n        demo.stop_containers()\n    except Exception as e:\n        print(f\"\\n❌ Demo failed: {e}\")\n        demo.stop_containers()\n    finally:\n        # Ensure cleanup\n        demo.stop_containers()\n\nif __name__ == \"__main__\":\n    import os\n    os.makedirs(\"load_test_results\", exist_ok=True)\n    \n    asyncio.run(main())